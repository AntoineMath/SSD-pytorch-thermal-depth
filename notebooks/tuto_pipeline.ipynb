{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# La pipeline : du Dataloader a l'entree du reseau"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'torch'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-582ebc0c15b8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mskimage\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mPIL\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mImage\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mimgaug\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0maugmenters\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0miaa\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'torch'"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import skimage\n",
    "import torch\n",
    "from PIL import Image\n",
    "from imgaug import augmenters as iaa\n",
    "from imgaug.augmentables.bbs import BoundingBox, BoundingBoxesOnImage\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as patches\n",
    "from matplotlib.collections import PatchCollection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Loader\n",
    "Le `Data Loader` est une classe de pytorch qui permet de charger les images du `Dataset` en retournant a chaque step un batch de (images, boxes, labels, difficulties).\n",
    "\n",
    "Le `Dataset` lit les fichiers `.json`.\n",
    "\n",
    "Ici, on se place dans le cas d'un batch d'une seule image (on n'a pas besoin du label ni de la difficulty)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'Image' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-195f7993ea16>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mimg_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'../img/sitting_salon.png'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mimage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mImage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'r'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0mbbox\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m16\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m13\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m34\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m46\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m16\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m15\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m37\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mimage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbbox\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'Image' is not defined"
     ]
    }
   ],
   "source": [
    "# Epoch p, step n : donnees retournees par le dataloader dans train.py\n",
    "img_path = '../img/sitting_salon.png'\n",
    "\n",
    "image = Image.open(img_path, mode='r')\n",
    "bbox = torch.Tensor([[16, 13, 34, 46], [3, 16, 15, 37]])\n",
    "image.size, bbox"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing\n",
    "Le preprocessing a lieu lorsque le `Dataloader` appel un batch d'image en utilisant la fonction `__getitem__()` du `Dataset`. \n",
    "\n",
    "**Chaque image est pass√©e a travers la fonction `thermal_image_preprocessing()` dans `utils.py`**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Standardization\n",
    "**On soustrait la moyenne et on divise par l'ecart type**. La moyenne et l'ecart type sont calcules sur tout le training set automatiquement grace a la fonction `dataset_mean_std()` de la classe Dataset.\n",
    "\n",
    "Pour cet exemple, on utilise la moyenne et l'ecart type de notre unique image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((60, 80, 1), 29294.899166666666, 158.94683282774847, 29049, 30253)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# PIL image to numpy array\n",
    "image = np.array(image)\n",
    "image = np.expand_dims(image, axis=-1)\n",
    "\n",
    "mean, std = image.mean(), image.std()\n",
    "image.shape, mean, std, image.min(), image.max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2.5431316e-08, 1.0, -1.547053, 6.027807)"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "image = (image - mean) / std\n",
    "image = image.astype('float32')\n",
    "image.mean(), image.std(), image.min(), image.max()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Augmentation\n",
    "L'image et la bbox associee sont passees dans la fonction `data_augmentation()` dans `utils.py`.\n",
    "\n",
    "J'utilise la librairie [imgaug](https://imgaug.readthedocs.io/en/latest/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BoundingBoxesOnImage([BoundingBox(x1=16.0000, y1=13.0000, x2=34.0000, y2=46.0000, label=None), BoundingBox(x1=3.0000, y1=16.0000, x2=15.0000, y2=37.0000, label=None)], shape=(60, 80))"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Creation de l'objet BoundingBoxesOnImage qui constitue la liste des bounding boxes de l'image\n",
    "list_box = []\n",
    "\n",
    "for box in bbox.tolist():\n",
    "    list_box.append(BoundingBox(*box))\n",
    "    \n",
    "bbs = BoundingBoxesOnImage(list_box, shape=image.shape)\n",
    "bbs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# On definie la liste des transformations a faire\n",
    "augmenters = [iaa.SomeOf((1, 3),\n",
    "                 [iaa.Crop(percent=(0.1, 0.2)),\n",
    "                  iaa.OneOf([iaa.Dropout(p=(0.01, 0.2)),\n",
    "                             iaa.CoarseDropout((0.03, 0.15), size_percent=(0.02, 0.05))]),\n",
    "                  iaa.Fliplr(1.0)],random_order=True)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transformation(image, bbs):\n",
    "    # On fixe une limite de 50 essais pour faire la transformation\n",
    "    #(si une boxe se retrouve en dehors de l'image par exemple)\n",
    "    for i in range(50):\n",
    "            seq = iaa.Sometimes(0.8, augmenters)\n",
    "            image_aug, bbs_aug = seq(image=image, bounding_boxes=bbs)\n",
    "\n",
    "            wrong_boxes = 0\n",
    "            for bb in bbs_aug.bounding_boxes:\n",
    "                bb_cliped_area = bb.clip_out_of_image(image_aug).area\n",
    "                bb_area = bb.area\n",
    "                # Si on crop plus de 20% d'une bbox lors du clip_out_of_image(), on recommence.\n",
    "                if bb_cliped_area / bb_area < 0.8:\n",
    "                    wrong_boxes += 1\n",
    "            # Si toutes les bbox de l'images sont a l'interieur de l'image, on garde cet essai\n",
    "            if wrong_boxes == 0:\n",
    "                bbs_aug = bbs_aug.clip_out_of_image()\n",
    "                break\n",
    "    return image_aug, bbs_aug"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((60, 80),\n",
       " BoundingBoxesOnImage([BoundingBox(x1=16.0000, y1=13.0000, x2=34.0000, y2=46.0000, label=None), BoundingBox(x1=3.0000, y1=16.0000, x2=15.0000, y2=37.0000, label=None)], shape=(60, 80)))"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Simulation de 10 steps : a chacune d'elles, l'image est transformee aleatoirement (ou pas). \n",
    "for i in range(10):\n",
    "    image_aug, bbs_aug = transformation(image, bbs)\n",
    "    \n",
    "image_aug.shape, bbs_aug\n",
    "\n",
    "#TODO : visualisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[16., 13., 34., 46.],\n",
       "        [ 3., 16., 15., 37.]])"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Pour recuperer la/les bounding box(es) transforme(es) sour la forme d'un tensor:\n",
    "boxes = []\n",
    "for nb in bbs_aug.bounding_boxes:\n",
    "            boxes.append([nb.x1, nb.y1, nb.x2, nb.y2])\n",
    "bbox = torch.FloatTensor(boxes)\n",
    "bbox"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Resizing\n",
    "On utilise le SSD300 donc son architecture est adaptee a des images `300x300`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(300, 300)"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "image = skimage.transform.resize(image_aug, (300, 300))\n",
    "image.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([300, 300]), tensor([[16., 13., 34., 46.],\n",
       "         [ 3., 16., 15., 37.]]))"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Format de base de pytorch : (Taille_batch, Channels, Widht, Height)\n",
    "image = np.moveaxis(image, -1, 0)\n",
    "image = torch.FloatTensor(image)\n",
    "image.shape, bbox"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fin du preprocessing\n",
    "Les images et les bounding boxes vont etre concatenees dans un batch et celui-ci sera passe a travers le `model()`.\n",
    "\n",
    "L'architecture du model se trouve dans `model.py`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
